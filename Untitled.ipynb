{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39962a45-0934-4fa6-89c9-1688aca380ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning titles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44898/44898 [05:32<00:00, 135.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning bodies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning Text: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44898/44898 [1:02:40<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Preview of cleaned data:\n",
      "                                               title  \\\n",
      "0  Moscow, Seoul closer on North Korea after thei...   \n",
      "1  NIGHTMARE SCENARIO: FOX NEWS Reports Obama Can...   \n",
      "2  Egypt says suspended U.S. military exercises t...   \n",
      "3  North Carolina transgender bathroom law faces ...   \n",
      "4   Mike Pence Gets A Special Gift In His Office ...   \n",
      "\n",
      "                                       combined_text  label  \n",
      "0  Moscow Seoul close North Korea leader meet RIA...      1  \n",
      "1  NIGHTMARE SCENARIO FOX NEWS report Obama Appoi...      0  \n",
      "2  Egypt say suspend U.S. military exercise resum...      1  \n",
      "3  North Carolina transgender bathroom law face f...      1  \n",
      "4    Mike Pence get Special Gift Office Mail Mont...      0  \n",
      "\n",
      "Cleaned dataset saved as 'cleanedfakenews1.csv'\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load CSV files\n",
    "fake_df = pd.read_csv(\"fake1.csv\")\n",
    "real_df = pd.read_csv(\"true1.csv\")\n",
    "\n",
    "# Add labels\n",
    "fake_df[\"label\"] = 0  # Fake news\n",
    "real_df[\"label\"] = 1  # Real news\n",
    "\n",
    "# Combine and shuffle datasets\n",
    "df = pd.concat([fake_df, real_df], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Efficient cleaning function using tqdm (NO file=sys.stdout)\n",
    "def clean_texts(texts):\n",
    "    cleaned = []\n",
    "    for doc in tqdm(nlp.pipe(texts, batch_size=50), total=len(texts), desc=\"ðŸ§¹ Cleaning Text\", leave=True):\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        cleaned.append(\" \".join(tokens))\n",
    "    return cleaned\n",
    "\n",
    "# Clean title and text columns (shows progress)\n",
    "print(\"Cleaning titles...\")\n",
    "df['clean_title'] = clean_texts(df['title'].astype(str))\n",
    "\n",
    "print(\"Cleaning bodies...\")\n",
    "df['clean_text'] = clean_texts(df['text'].astype(str))\n",
    "\n",
    "# Combine cleaned title and text\n",
    "df['combined_text'] = df['clean_title'] + \" \" + df['clean_text']\n",
    "\n",
    "# Preview the cleaned dataset\n",
    "print(\"\\n Preview of cleaned data:\")\n",
    "print(df[['title', 'combined_text', 'label']].head())\n",
    "\n",
    "# Save to a new CSV file\n",
    "df.to_csv(\"cleanedfakenews1\", index=False)\n",
    "print(\"\\nCleaned dataset saved as 'cleanedfakenews1.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86df4a28-cd71-4f08-a6ba-178629f6b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import this\n",
    "\n",
    "# Splitting the data\n",
    "X1 = df['combined_text']  # Features (cleaned text)\n",
    "Y1 = df['label']          # Labels (0 for fake, 1 for real)\n",
    "\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(\n",
    "    X1, Y1, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d080135-47c9-4f1b-aa80-4211d9fb9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b21a287f-9e17-4d6e-b7d1-ebaf451098bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer\n",
    "vectorizer1 = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform training data\n",
    "X1_train_vec = vectorizer1.fit_transform(X1_train)\n",
    "\n",
    "# Transform test data\n",
    "X1_test_vec = vectorizer1.transform(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a5f548-e084-4e85-ab58-6f4f9d612b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape for X_train: (35918, 5000)\n",
      "TF-IDF shape for X_test: (8980, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the vectors\n",
    "print(\"TF-IDF shape for X_train:\", X1_train_vec.shape)\n",
    "print(\"TF-IDF shape for X_test:\", X1_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3a51d4-a71e-49b8-8efd-243f605a2a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y1_test.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"cleanedfakenews1.csv\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer1, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "# Save data splits (optional but helpful)\n",
    "joblib.dump(X1_train_vec, 'X1_train_vec.pkl')\n",
    "joblib.dump(X1_test_vec, 'X1_test_vec.pkl')\n",
    "joblib.dump(Y1_train, 'Y1_train.pkl')\n",
    "joblib.dump(Y1_test, 'Y1_test.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2865dcbd-da44-47b0-a3d0-48b4506fc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "074d2a0b-3c26-424c-9a87-b783c1d9f909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.9865256124721603\n",
      "Confusion Matrix:\n",
      " [[4628   68]\n",
      " [  53 4231]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      4696\n",
      "           1       0.98      0.99      0.99      4284\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#logistuc regression \n",
    "log_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
    "log_model.fit(X1_train_vec, Y1_train)\n",
    "log_preds = log_model.predict(X1_test_vec)\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y1_test, log_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y1_test, log_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(Y1_test, log_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9910252-4429-467e-8d5b-3533fbc74383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Results:\n",
      "Accuracy: 0.9256124721603564\n",
      "Confusion Matrix:\n",
      " [[4393  303]\n",
      " [ 365 3919]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      4696\n",
      "           1       0.93      0.91      0.92      4284\n",
      "\n",
      "    accuracy                           0.93      8980\n",
      "   macro avg       0.93      0.93      0.93      8980\n",
      "weighted avg       0.93      0.93      0.93      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X1_train_vec, Y1_train)\n",
    "nb_preds = nb_model.predict(X1_test_vec)\n",
    "\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y1_test, nb_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y1_test, nb_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(Y1_test, nb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e311562f-cd8a-4212-9658-0da93b8f3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9972160356347439\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       1.00      1.00      1.00      4696\n",
      "        Real       1.00      1.00      1.00      4284\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n",
      "\n",
      "Confusion Matrix (rows=true, cols=predicted):\n",
      "             Pred Fake  Pred Real\n",
      "Actual Fake       4681         15\n",
      "Actual Real         10       4274\n",
      "\n",
      "Saved XGBoost model as 'xgboost_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 2. Initialize XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train\n",
    "xgb_model.fit(X1_train_vec, Y1_train)\n",
    "\n",
    "# 4. Predict\n",
    "xgb_pred = xgb_model.predict(X1_test_vec)\n",
    "\n",
    "# 5. Evaluate\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(Y1_test, xgb_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y1_test, xgb_pred, target_names=[\"Fake\", \"Real\"]))\n",
    "\n",
    "cm = confusion_matrix(Y1_test, xgb_pred, labels=[0,1])\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=predicted):\")\n",
    "print(pd.DataFrame(cm,\n",
    "                   index=[\"Actual Fake\",\"Actual Real\"],\n",
    "                   columns=[\"Pred Fake\",\"Pred Real\"]))\n",
    "\n",
    "# 6. Save trained model\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "print(\"\\nSaved XGBoost model as 'xgboost_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae712fe4-46bd-4012-aa5d-83e3919abbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 17133, number of negative: 18785\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.472458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 737187\n",
      "[LightGBM] [Info] Number of data points in the train set: 35918, number of used features: 4998\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "\n",
      "LightGBM Results:\n",
      "Accuracy: 0.9972160356347439\n",
      "Confusion Matrix:\n",
      " [[4680   16]\n",
      " [   9 4275]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4696\n",
      "           1       1.00      1.00      1.00      4284\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\conne\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "lgb_model.fit(X1_train_vec, Y1_train)\n",
    "lgb_preds = lgb_model.predict(X1_test_vec)\n",
    "\n",
    "print(\"\\nLightGBM Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(Y1_test, lgb_preds))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y1_test, lgb_preds))\n",
    "print(\"Classification Report:\\n\", classification_report(Y1_test, lgb_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d6df0c4-4de4-4be7-b5c8-45c6dd8c93b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lightgbm_model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(log_model, 'logistic_model.pkl')\n",
    "joblib.dump(nb_model, 'naive_bayes_model.pkl')\n",
    "joblib.dump(xgb_model, 'xgboost_model.pkl')\n",
    "joblib.dump(lgb_model, 'lightgbm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d8ea562-b760-4b68-ba7c-8b14f7fb7802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Predictions:\n",
      "Logistic Regression: Real\n",
      "Naive Bayes: Real\n",
      "XBBOOST: Fake\n",
      "LightGBM: Fake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\conne\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for preprocessing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load saved models\n",
    "log_model = joblib.load(\"logistic_model.pkl\")\n",
    "nb_model = joblib.load(\"naive_bayes_model.pkl\")\n",
    "xgb_model = joblib.load(\"xgboost_model.pkl\")\n",
    "lgb_model = joblib.load(\"lightgbm_model.pkl\")\n",
    "\n",
    "# Load TF-IDF vectorizer\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Text preprocessing function\n",
    "def clean_input(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# User input\n",
    "user_title = input(\"Enter the news title: \")\n",
    "user_text = input(\"Enter the news body: \")\n",
    "\n",
    "# Combine and clean\n",
    "combined_text = user_title + \" \" + user_text\n",
    "cleaned_text = clean_input(combined_text)\n",
    "\n",
    "# Vectorize\n",
    "vectorized_input = vectorizer.transform([cleaned_text])\n",
    "\n",
    "# Predictions\n",
    "print(\"\\nðŸ§  Predictions:\")\n",
    "print(\"Logistic Regression:\", \"Real\" if log_model.predict(vectorized_input)[0] == 1 else \"Fake\")\n",
    "print(\"Naive Bayes:\", \"Real\" if nb_model.predict(vectorized_input)[0] == 1 else \"Fake\")\n",
    "print(\"XBBOOST:\", \"Real\" if xgb_model.predict(vectorized_input)[0] == 1 else \"Fake\")\n",
    "print(\"LightGBM:\", \"Real\" if lgb_model.predict(vectorized_input)[0] == 1 else \"Fake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e700042-9251-4c04-ad68-053bbaf180f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
